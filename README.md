[Machine Learning (CSCI 1420/ENGN 2520)](http://cs.brown.edu/~pff/engn2520/calendar.html)
---
Homeworks 60%
Midterm 20%  (Thursday April 6 in class)
Final 20%
---
<html>
<body>
<a href = "index.html">Home</a> &nbsp;
<a href = "hw.html">Assignments</a> &nbsp;
<a href = "calendar.html">Calendar</a> &nbsp;
<a href = "matlab.html">Matlab</a>

<hr>

<h3>Lecture calendar</h3>
<table width="95%" border="1" cellpadding="5"
       cellspacing="0" align="center">
              
  <tr>
    <th width="5%" id="header">Lecture</td>
    <th width="10%" id="header">Date</td>
    <th width="40%" id="header">Topic</td>
    <th width="20%" id="header">Lecture notes</td>
    <th width="35%" id="header">Reference (book sections)</td>
  </tr>

  <tr>
    <td><p>1</p></td>
    <td><p>January 26</p></td>
    <td><p>Introduction</p></td>
    <td><a href="CS1420_Lecture_1.pdf">notes</a></td>
    <td><p></p></td>
  </tr>

  <tr>
    <td><p>2</p></td>
    <td><p>January 31</p></td>
    <td><p>Linear regression, basis functions, least squares</p></td>
    <td><a href="CS1420_Lecture_2.pdf">notes</a></td>
    <td><p>1.1, 3.1</p></td>
  </tr>

  <tr>
    <td><p>3</p></td>
    <td><p>February 2</p></td>
    <td><p>Special DSI lecture, Chris Danforth (UVM)</p></td>
    <td><a href="#"></a></td>
    <td></td>
  </tr>

   <tr>
    <td><p>4</p></td>
    <td><p>February 7</p></td>
    <td><p>Maximum likelihood view of linear regression, outliers</p></td>
    <td><a href="CS1420_Lecture_3.pdf">notes</a></td>
    <td><p>3.1</p></td>
  </tr>

  <tr>
    <td><p>5</p></td>
    <td><p>February 14</p></td>
    <td><p>Robust regression and Linear Programming</p></td>
    <td><a href="LADLP.pdf">pff's notes</a>, <a href="CS1420_Lecture_4.pdf">regular notes</a></td>
    <td><p></p></td>
  </tr>

  <tr>
    <td><p>6</p></td>
    <td><p>February 16</p></td>
    <td><p>Classification, Bayesian Decision Theory</p></td>
    <td><a href="CS1420_Lecture_5.pdf">notes</a></td>
    <td><p>1.5</p></td>
  </tr>

  <tr>
    <td><p>7</p></td>
    <td><p>February 23</p></td>
    <td><p>Estimating distributions (parametric and non-parametric)</p></td>
    <td><a href="CS1420_Lecture_6.pdf">notes</a></td>
    <td><p>2.1, 2.2, 2.3, 2.5</p></td>
  </tr>

  <tr>
    <td><p>8</p></td>
    <td><p>February 28</p></td>
    <td><p>Parzen windows, Bayesian estimation, predictive distribution </p></td>
    <td><a href="CS1420_Lecture_7.pdf">notes</a></td>
    <td><p>2.1, 2.2, 2.3, 2.5</p></td>
  </tr>

  <tr>
    <td><p>9</p></td>
    <td><p>March 2</p></td>
    <td><p>Linear separators, Perceptron Algorithm</p></td>
    <td><a href="CS1420_Lecture_9.pdf">notes</a></td>
    <td><p>4.1, 4.1.7</p></td>
  </tr>

  <tr>
    <td><p>10</p></td>
    <td><p>March 7</p></td>
    <td><p>Max-margin separator, linear support vector machines</p></td>
    <td><a href="CS1420_Lecture_10.pdf">notes</a></td>
    <td><p>7.1</p></td>
  </tr>

  <tr>
    <td><p>11</p></td>
    <td><p>March 9</p></td>
    <td><p>Gradient descent for linear SVM, Multiclass problems</p></td>
    <td><a href="CS1420_Lecture_10.pdf"></a></td>
    <td><p>7.1</p></td>
  </tr>

</table>
<p>

</body>
</html>
